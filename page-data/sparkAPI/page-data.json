{"componentChunkName":"component---src-templates-docs-js","path":"/sparkAPI","result":{"data":{"site":{"siteMetadata":{"title":"Data Engineering at Central European University","docsLocation":"https://github.com/zoltanctoth/ceu-cloud-bigdata-course/tree/master"}},"mdx":{"fields":{"id":"dcabc600-eff5-5241-943a-cad298f41d01","title":"Week 6 - Apache Spark APIs","slug":"/sparkAPI"},"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Week 6 - Apache Spark APIs\",\n  \"metaTitle\": \"Week 6 - Apache Spark APIs\",\n  \"metaDescription\": \"Week 6 - Apache Spark APIs\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"The Spark DataFrame and SQL API\"), mdx(\"p\", null, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/346304/2168141618055043/484361/latest.html\"\n  }), \"A Gentle Introduction to Apache Spark on Databricks\")), mdx(\"p\", null, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/346304/2168141618055109/484361/latest.html\"\n  }), \"Apache Spark on Databricks for Data Engineers\")), mdx(\"h1\", null, \"Fundamental concepts:\"), mdx(\"p\", null, \"Spark is a distributed programming model in which the user specifies transformations. These transformations build up a directed acyclic graph (DAG) of transformations and action. An action begins the process of execution that graph of instructions, as a single job, by breaking it down into stages and tasks to execute across the cluster. The logical structures that we manipulate with transformations and actions are DataFrames and Datasets. To create a new DataFrame or Dataset, you call a transformation. To start computation or convert to native language types, you call an action.\"), mdx(\"h1\", null, \"Structured API Overview\"), mdx(\"p\", null, \"Structured APIs are a tool for manipulating all sorts of data, from unstructured log files to semi-structured CSV files and highly structured Parquet (Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language) files. These APIs refer to three core types of distributed collection APIs:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Datasets\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"DataFrames\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"SQL Tables and Views\")), mdx(\"p\", null, \"Majority of the Structured APIs apply to both batch and streaming computation. This means that when you work with the Structured APIs, it should be simple to migrate from batch to streaming or vice versa with little to no effort.\\xA0Structured APIs are the fundamental abstraction used to write the majority of data flows.\\xA0\"), mdx(\"h1\", null, \"DataFrames and Datasets\"), mdx(\"p\", null, \"DataFrames and Datasets are distributed table-like collection with well-defined rows and columns. Each column must have the same number of rows as all the other columns although\\xA0you can use null to specify the absence of a value and each column has type information that must be consistent for every row in the collection. To Spark, DataFrames and Datasets represent immutable, lazily evaluated plans(transformations)\\xA0that specify what operations to apply to data residing at a location to generate some output. When we perform an action on a DataFrame, we instruct Spark to perform the actual transformations and return the results. These represent plans of how to manipulate rows and columns to compute the user's desired result.\\xA0\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Schema:\"), \" defines the column names and types of a DataFrame. You can define schemas manually or read a schema from a data source. Schemas consist of types meaning that you need a way of specifying what lies where.\"), mdx(\"p\", null, \"Spark is effectively a programming language of its own.\\xA0Spark uses an engine called Catalyst:\\n\", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Catalyst:\"), \" maintains its own type information through the planning and processing of work. This opens up execution optimizations. \"), mdx(\"h1\", null, \"Structured APIs\"), mdx(\"table\", null, mdx(\"thead\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"thead\"\n  }, mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"DataFrame\"), mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"DataSet\"))), mdx(\"tbody\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"untyped (Spark maintains them completely)\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": null\n  }), \"typed (checks whether types conform to the specification at compile time.) Only available to JVM based languages Scala and Java.\")))), mdx(\"p\", null, \"DataFrames are Datasets of \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Type Row.\"), \" The \\\"Row\\\" type is Spark's internal representation of its optimized in-memory format for computation.\\xA0To Spark in Python and R there is no such thing as a Dataset, everything is a DataFrame and therefore we always operate on that optimized format.\"), mdx(\"h1\", null, \"Overview of Structured API Execution\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Write DataFrame/Dataset/SQL Code\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"If valid code, Spark converts this to a Logical Plan\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Spark transforms this Logical Plan, checking for optimizations along the way\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Spark then executes this Physical Plan on the cluster\")), mdx(\"h1\", null, \"Logical Planning\"), mdx(\"p\", null, \"The first phase takes user code and converts it into a logical plan (optimized version of the user's set of expressions)\\nIt does this by converting user code into an unresolved logical plan. The plan is unresolved because although your code might be valid, the tables and columns that it refers to might or might not exist. Spark uses the catalog, a repository of all table and DataFrame information to resolve columns and tables in the analyzer. The analyzer might reject the unresolved logical plan if the required column name does not exist in the catalog. If the analyzer can resolve it, the result is passed through the Catalyst Optimizer, a collection of rules that attempt to optimize the logical plan by pushing down predicates or selections. Packages can extend the Catalyst to include their own rules for domain-specific optimizations.\"), mdx(\"h1\", null, \"Physical Planning\"), mdx(\"p\", null, \"After creating the optimized logical plan, Spark begins the physical planning process. The physical plan - often called Spark plan - specifies how the logical plan will execute on the cluster by generating different physical execution strategies and comparing them through a cost model. (Upon selecting a physical plan Spark runs all of this code over RDDs.)\"), mdx(\"h1\", null, \"Basic Structured Operations\"), mdx(\"p\", null, \"DataFrame consists of a series of records that are of type Row and a number of columns that represent a computation expression that can perform on each individual record in the Dataset. Schemas define the name as well as the type of data in each column.\\xA0Partitioning\\xA0of the DataFrame defines the layout of the DataFrame or Dataset's physical distribution across the cluster. The partitioning scheme defines how that is allocated.\"), mdx(\"h1\", null, \"DataFrame\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Create DataFrame:\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"df = spark.read.format(\\\"json/csv/..\\\").load(\\\"Some/path\\\")\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Print Schema:\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"printSchema\\ndf.printSchema()\\n\")), mdx(\"h1\", null, \"Columns and Expressions\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Expressions - when you select, remove manipulate columns from DataFrames\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Create Column:\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"from pyspark.sql.functions import col, column\\ncol(\\\"someColumnName\\\")\\ncolumn(\\\"someColumnName\\\")\\n\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Explicit Column References\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"If you need to refer to a specific DataFrame's column, you can use the col method on the specific DataFrame. This can be useful when performing a join and need to refer to a specific column in one DataFrame that might share the same name with another column in the joined DataFrame.\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Expressions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Columns are expressions. An expression is a set of transformations on one or more values in a record in a DataFrame.\\xA0\\nAn expression created via the expr function is just a DataFrame column reference.\\nexpr(\\\"someCol\\\") is equivalent to\\xA0col(\\\"someCol\\\")\")), mdx(\"h1\", null, \"Records and Rows\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Each row in a DataFrame is a single  record as an object type Row.\\xA0\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Spark manipulates Row objects using column expressions in order to produce usable values.\\xA0\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Create Rows:\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"from pyspark.sql import Row\\nmyRow = Row(\\\"Hello\\\", None, 1, False)\\n\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"DataFrame Transformations\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"We can add rows or columns\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"We can remove rows and columns\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"We can transform a row into a column or vice versa\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"We can change the order of rows based on the values in columns\")), mdx(\"h1\", null, \"DataFrame API Example Using Different types of Functionality\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reading Data:\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"df = spark.read \\\\\\n  .option(\\\"header\\\", True) \\\\\\n  .option(\\\"sep\\\", \\\",\\\") \\\\\\n  .option(\\\"inferSchema\\\", True) \\\\\\n  .csv(\\\"PATH/file.csv\\\") \\n\")), mdx(\"h2\", null, \"show()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"If you want to see top 20 rows of DataFrame in a tabular form then use the following command.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/showdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"show(n)\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"If you want to see n  rows of DataFrame in a tabular form then use the following command.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.show(2)\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/shown.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"take()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"take(n) Returns the first n rows in the DataFrame.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.take(2).foreach(println)\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/takedata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"count()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns the number of rows.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.groupBy(\\\"speed\\\").count().show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/countdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"head()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"head () is used to returns first row.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\nresultHead = carDataFrame.head()\\n\\n    println(resultHead.mkString(\\\",\\\"))\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/headdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"head(n)\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"head(n) returns first n rows.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"resultHeadNo = carDataFrame.head(3)\\n\\n    println(resultHeadNo.mkString(\\\",\\\"))\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/headn.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"first()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns the first row.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"resultFirst = carDataFrame.first()\\n\\n    println(\\\"fist:\\\" + resultFirst.mkString(\\\",\\\"))\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/firstdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"collect()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns an array that contains all of Rows in this DataFrame.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\nresultCollect = carDataFrame.collect()\\n\\n    println(resultCollect.mkString(\\\",\\\"))\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/collectdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"Basic DataFrame functions:\"), mdx(\"h2\", null, \"printSchema()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"If you want to see the Structure (Schema) of the DataFrame, then use the following command.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.printSchema()\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/printschemadata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"toDF()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \" toDF() Returns a new DataFrame with columns renamed. It  can be quite convenient in conversion from a RDD of tuples into a DataFrame with meaningful names.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncar = sc.textFile(\\\"src/main/resources/fruits.txt\\\")\\n      .map(_.split(\\\",\\\"))\\n      .map(f => Fruit(f(0).trim.toInt, f(1), f(2).trim.toInt))\\n      .toDF().show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/todfdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"dtypes()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns all column names and their data types as an array.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.dtypes.foreach(println)\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/dtypesdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"columns ()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns all column names as an array.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.columns.foreach(println)\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/columnsdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"cache()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \" cache() explicitly to store the data into memory. Or data stored in a distributed way in the memory by default.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"resultCache = carDataFrame.filter(carDataFrame(\\\"speed\\\") > 300)\\n\\n    resultCache.cache().show()\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/cachedata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"Data Frame operations:\"), mdx(\"h2\", null, \"sort()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns a new DataFrame sorted by the given expressions.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.sort($\\\"itemNo\\\".desc).show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/shortdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"orderBy()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns a new DataFrame sorted by the specified column(s).\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.orderBy(desc(\\\"speed\\\")).show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/orderbydata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"groupBy()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"counting the number of cars who are of the same speed .\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.groupBy(\\\"speed\\\").count().show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/groupbydata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"na()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns a DataFrameNaFunctions for working with missing data.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.na.drop().show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/nadata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"as()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns a new DataFrame with an alias set.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"carDataFrame.select(avg($\\\"speed\\\").as(\\\"avg_speed\\\")).show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/asdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"alias()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns a new DataFrame with an alias set. Same as \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"as\"), \".\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.select(avg($\\\"weight\\\").alias(\\\"avg_weight\\\")).show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/aliasdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"select()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"To fetch speed-column among all columns from the DataFrame.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.select(\\\"speed\\\").show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/selectdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"filter()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"filter the cars whose speed is greater than 300 (speed > 300).\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.filter(carDataFrame(\\\"speed\\\") > 300).show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/filterdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"where()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Filters age using the given SQL expression.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"carDataFrame.where($\\\"speed\\\" > 300).show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/wheredata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"agg()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Aggregates on the entire DataFrame without groups.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.agg(max($\\\"speed\\\")).show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/aggdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"limit()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns a new DataFrame by taking the first n rows.The difference between this function and head is that head returns an array while limit returns a new DataFrame.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame1.limit(3).show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/limit.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"unionAll()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns a new DataFrame containing union of rows in this frame and another frame.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"carDataFrame.unionAll(empDataFrame2).show()\\n\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/unionalldata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"intersect()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns a new DataFrame containing rows only in both this frame and another frame.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame1.intersect(carDataFrame).show()\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/intersectdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"except()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns a new DataFrame containing rows in this frame but not in another frame.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.except(carDataFrame1).show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/exceptdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"withColumnRenamed()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns a new DataFrame with a column renamed.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\nempDataFrame2.withColumnRenamed(\\\"id\\\", \\\"employeeId\\\").show()\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/withcolumnrenameddata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"drop()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns a new DataFrame with a column dropped.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.drop(\\\"speed\\\").show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/dropdata.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"dropDuplicates()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Returns a new DataFrame that contains only the unique rows from this DataFrame.\\nThis is an alias for distinct.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.dropDuplicates().show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/dropduplicates.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"describe()\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"describe returns a DataFrame containing information such as number of non-null entries (count),mean, standard deviation, and minimum and maximum value for each numerical column.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.describe(\\\"speed\\\").show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/describe.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"Show the Data\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"If you want to see the data in the DataFrame, then use the following command.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.show()\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/show.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"printSchema Method\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"If you want to see the Structure (Schema) of the DataFrame, then use the following command\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \" carDataFrame.printSchema()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/schema.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"Select Method\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Use the following command to fetch name-column among three columns from the DataFrame\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.select(\\\"name\\\").show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/select.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h2\", null, \"Filter Method\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Use the following command to filter whose speed is greater than 300 (speed > 300)from the DataFrame\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-scala\"\n  }), \"\\ncarDataFrame.filter(carDataFrame(\\\"speed\\\") > 300).show()\\n\\n\")), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://github.com/rklick-solutions/spark-tutorial/wiki/images/filterage.png\",\n    \"alt\": \"alt text\"\n  }))), mdx(\"h1\", null, \"Working with Different Types of Data\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Objectives\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Booleans\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Numbers\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Strings\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Dates and Timestamps\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Handling Null\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Complex Types\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"User Defined Functions\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Where to look for APIs?\"), \"\\nDataFrame (Dataset) Methods: DataFrame is just a Dataset of Row types, so you will look at the Dataset methods available at:\\xA0\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset\"\n  }), \"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset\"), \"\\nDataFrameStatFunctions:\\xA0\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions\"\n  }), \"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions\"), \"\\nDataFrameNaFunctions\\xA0\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions\"\n  }), \"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions\"), \"\\nColumn methods:\\xA0\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column\"\n  }), \"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column\"), \"\\nFunctions:\\xA0\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"\n  }), \"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"), \"\\nDataset:\\xA0\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset\"\n  }), \"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset\")), mdx(\"h1\", null, \"Joins\"), mdx(\"p\", null, \"Joins bring together a large number of different datasets. It helps avoid OutOfMemory issues and solve problems.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Join Expressions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Join expressions determine whether two rows should join\\nA join brings together two sets of data, the left and the right, by comparing the value of one or more keys of the left and right and evaluating the result of a join expression that determines whether Spark should bring together the left set of data with the right set of data.\\xA0\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Join types\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"The join type determines what should be in the result set.\\xA0\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Inner joins (keep rows with keys that exist in the left and right datasets)\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Outer joins (keep rows with keys in either the left or right datasets)\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Left outer joins (keep rows with keys in the left dataset)\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Right outer joins (keep rows with keys in the right dataset)\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Left semi joins (keep the rows in the left, and only the left, dataset where the key\\xA0appears in the right dataset)\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Left anti joins (keep the rows in the left, and only the left, dataset where they dos\\xA0not appear in the right dataset)\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Natural joins (perform a join by implicitly matching the columns between the\\xA0two datasets with the same names)\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Cross (or cartesian) joins (match every row in the left dataset with every row in\\xA0the right dataset)\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Create a simple dataset:\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"person = spark.createDataFrame([\\n(0, \\\"Bill Chambers\\\", 0, [100]),\\n(1, \\\"Matei Zaharia\\\", 1, [500, 250, 100]),\\n(2, \\\"Michael Armbrust\\\", 1, [250, 100])])\\\\\\n.toDF(\\\"id\\\", \\\"name\\\", \\\"graduate_program\\\", \\\"spark_status\\\")\\ngraduateProgram = spark.createDataFrame([\\n(0, \\\"Masters\\\", \\\"School of Information\\\", \\\"UC Berkeley\\\"),\\n(2, \\\"Masters\\\", \\\"EECS\\\", \\\"UC Berkeley\\\"),\\n(1, \\\"Ph.D.\\\", \\\"EECS\\\", \\\"UC Berkeley\\\")])\\\\\\n.toDF(\\\"id\\\", \\\"degree\\\", \\\"department\\\", \\\"school\\\")\\nsparkStatus = spark.createDataFrame([\\n(500, \\\"Vice President\\\"),\\n(250, \\\"PMC Member\\\"),\\n(100, \\\"Contributor\\\")])\\\\\\n.toDF(\\\"id\\\", \\\"status\\\")\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Register these tables:\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"person.createOrReplaceTempView(\\\"person\\\")\\ngraduateProgram.createOrReplaceTempView(\\\"graduateProgram\\\")\\nsparkStatus.createOrReplaceTempView(\\\"sparkStatus\\\")\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Inner Joins\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Evaluate the keys in both of the DataFrames or tables and include only the rows that evaluate to true.\\xA0Join graduateProgram DataFrame with the person DataFrame.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"joinExpression = person[\\\"graduate_program\\\"] == graduateProgram['id']\\n#keys don't exist in both DataFrames so\\xA0the following expression results in zero values.\\nwrongJoinExpression = person[\\\"name\\\"] == graduateProgram[\\\"school\\\"]\\nperson.join(graduateProgram, joinExpression).show()\\n\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"We can also specify this explicitly by passing in a third parameter, the joinType.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"joinType = \\\"inner\\\"\\nperson.join(graduateProgram, joinExpression, joinType).show()\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Outer Joins\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Evaluate the keys in both of the DataFrames or tables and includes (and\\xA0joins together) the rows that evaluate to true or false. If there is no equivalent row in\\xA0either the left or right DataFrame, Spark will insert null.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"joinType = \\\"outer\\\"\\nperson.join(graduateProgram, joinExpression, joinType).show()\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Left Outer Joins\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Left outer joins evaluate the keys in both of the DataFrames or tables and includes all\\xA0rows from the left DataFrame as well as any rows in the right DataFrame that have a\\xA0match in the left DataFrame. If there is no equivalent row in the right DataFrame,\\xA0Spark will insert null.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"joinType = \\\"left_outer\\\"\\ngraduateProgram.join(person, joinExpression, joinType).show()\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Right Outer Joins\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Right outer joins evaluate the keys in both of the DataFrames or tables and includes\\xA0all rows from the right DataFrame as well as any rows in the left DataFrame that have\\xA0a match in the right DataFrame. If there is no equivalent row in the left DataFrame,\\xA0Spark will insert null.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"joinType = \\\"right_outer\\\"\\nperson.join(graduateProgram, joinExpression, joinType).show()\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Left Semi Joins\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Semi joins do not actually include\\xA0any values from the right DataFrame. They only compare values to see if the value\\xA0exists in the second DataFrame. If the value does exist, those rows will be kept in the\\xA0result, even if there are duplicate keys in the left DataFrame. Think of left semi joins\\xA0as filters on a DataFrame, as opposed to the function of a conventional join.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"joinType = \\\"left_semi\\\"\\ngraduateProgram.join(person, joinExpression, joinType).show()\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Left Anti Joins\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Left anti joins are the opposite of left semi joins. Like left semi joins, they do not\\xA0actually include any values from the right DataFrame. They only compare values to\\xA0see if the value exists in the second DataFrame. However, rather than keeping the values that exist in the second DataFrame, they keep only the values that do not have a\\xA0corresponding key in the second DataFrame. Like a NOT IN SQL\\xA0style filter.\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"joinType = \\\"left_anti\\\"\\ngraduateProgram.join(person, joinExpression, joinType).show()\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Cross (Cartesian) Joins\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Cross-joins\\xA0are inner joins that do not specify a predicate. Cross joins will join every single\\xA0row in the left DataFrame to ever single row in the right DataFrame. This will cause an absolute explosion in the number of rows contained in the resulting DataFrame. If\\xA0you have 1,000 rows in each DataFrame, the cross join of these will result in 1,000,000 (1,000 x 1,000) rows. For this reason, you must very explicitly state that you want a\\xA0cross-join by using the following special syntax:\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"joinType = \\\"cross\\\"\\ngraduateProgram.join(person, joinExpression, joinType).show()\\n\")), mdx(\"h1\", null, \"Challenges When Using Joins\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Joins on Complex Types\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Any expression is a\\xA0valid join expression, assuming that it returns a Boolean.\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Handling Duplicate Column Names\"), \"\\nIn a DataFrame, each column has a unique ID within Spark\\u2019s SQL Engine, Catalyst. This unique ID is purely internal and not something\\xA0that you can directly reference. This makes it quite difficult to refer to a specific column when you have a DataFrame with duplicate column names.\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"This can occur in two distinct situations:\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The join expression that you specify does not remove one key from one of the\\xA0input DataFrames and the keys have the same column name\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Two columns on which you are not performing the join on have the same name\")), mdx(\"p\", null, \"Note: If you partition your data\\xA0correctly prior to a join, you can end up with much more efficient execution because\\xA0even if a shuffle is planned, if data from two different DataFrames is already located\\xA0on the same machine, Spark can avoid the shuffle.\"), mdx(\"h1\", null, \"Data Sources\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"JSON:\"), \" JSON objects have structure, and JavaScript (on\\xA0which JSON is based) has at least basic types. This makes it easier to work with\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Parquet\"), \": Parquet is an open source column-oriented data store that provides a variety of storage optimizations, especially for analytics workloads. It provides columnar compression, which saves storage space and allows for reading individual columns instead of\\xA0entire files. It is\\xA0the default file format. Writing data out to Parquet for long-term storage because reading from a parquet file will always be more efficient than\\xA0JSON or CSV. Another advantage of Parquet is that it supports complex types. \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"ORC\"), \": ORC is a self-describing, type-aware columnar file format designed for Hadoop\\xA0workloads. It is optimized for large streaming reads, but with integrated support for\\xA0finding required rows quickly. ORC has no options for reading in data\\xA0because Spark understands the file format well.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"JDBC/ODBC Connections\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Plain Text Files\"), \" Each line in the file becomes a record\\xA0in the DataFrame.\\xA0\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"CSV:\"), \" Commma-Separated Values. Each line represents a single record, and commas separate each field within a\\xA0record. \")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Community-created Data Sources\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Cassandra\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"HBase\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"MongoDB\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"AWS Redshift\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"XML\")), mdx(\"h1\", null, \"The Structure of the Data Sources API\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reading data:\"), \"\\n\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"DataFrameReader.format(...).option(\\\"key\\\", \\\"value\\\").schema(...).load()\"), \"\\n#\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"format\"), \" is optional because Spark will use the parquet format by default\\n#\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \".option\"), \" allows you to set key value configurations to parameterize how you will read data\\n#\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"schema\"), \" is\\xA0optional if the data source provides a schema or if intend to use schema inference.\\xA0\"), mdx(\"p\", null, \"The foundation for reading data in Spark is the DataFrameReader. We access this\\xA0through the SparkSession via the read attribute:\\xA0\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"spark.read\"), \"\\nAfter we have a DataFrame reader, we specify several values:\\xA0\\nthe format (1)\\nthe\\xA0schema (2)\\nthe read mode (3)\\na series of options (4)\"), mdx(\"p\", null, \"Example: \"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"spark.read.format(\\\"csv\\\")\\n.option(\\\"mode\\\", \\\"FAILFAST\\\")\\n.option(\\\"inferSchema\\\", \\\"true\\\")\\n.option(\\\"path\\\", \\\"path/to/file(s)\\\")\\n.schema(someSchema)\\n.load()\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Read Mode\"), \"\\nReading data from an external source naturally entails encountering malformed data, especially when working with only semi-structured data sources. Read modes specify what will happen when Spark does come across malformed records.\"), mdx(\"h1\", null, \"Basics of Writing Data\"), mdx(\"p\", null, \"The foundation for writing data is quite similar to that of reading data. Instead of the\\xA0\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"DataFrameReader\"), \", we have the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"DataFrameWriter\"), \". Because we always need to write\\xA0out some given data source, we access the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"DataFrameWriter\"), \" on a per DataFrame basis\\xA0via the write attribute:\\xA0dataFrame.write\"), mdx(\"p\", null, \"\\xA0\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Specify three values:\"), \" the format, a series of\\xA0options, and the save mode. At a minimum, you must supply a path.\\xA0\\nExample:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"dataframe.write.format(\\\"csv\\\")\\n.option(\\\"mode\\\", \\\"OVERWRITE\\\")\\n.option(\\\"dateFormat\\\", \\\"yyyy-MM-dd\\\")\\n.option(\\\"path\\\", \\\"path/to/file(s)\\\")\\n.save()\\n\")), mdx(\"h1\", null, \"Save Mode\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Save modes specify what will happen if Spark finds data at the specified location.\\xA0The default is errorIfExists. This means that if Spark finds data at the location to\\xA0which you\\u2019re writing, it will fail the write immediately.\")), mdx(\"h1\", null, \"SparkSQL\"), mdx(\"p\", null, \"With SparkSQL you can:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Run SQL queries against views or tables organized into databases\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Use system functions of define user functions and analyze query plans in order to optimize their workloads\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"You can choose to express some of your data manipulations in SQL and others in DataFrames and they will compile to the same underlying code\")))), mdx(\"h1\", null, \"What is SQL?\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Structured Query Language\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Domain-specific language (A domain-specific language is created specifically to solve problems in a particular domain and is not intended to be able to solve problems outside it (although that may be technically possible). \", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"In contrast, general-purpose languages are created to solve problems in many domains.)\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"For expressing relational operations over data, it is used in all relational databases.\")), mdx(\"h1\", null, \"Spark SQL\"), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"The power of Spark SQL derives from:\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"SQL analysts can take\\xA0advantage of Spark\\u2019s computation abilities by plugging into the Thrift Server or\\xA0Spark\\u2019s SQL interface\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Data engineers and scientists can use Spark SQL where\\xA0appropriate in any data flow. \"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"This unifying API allows for data to be extracted with\\xA0SQL, manipulated as a DataFrame, passed into one of Spark MLlibs large scale\\xA0machine learning algorithms, written out to another data source and everything in\\xA0between.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Spark SQL is intended to operate as a online analytic processing\\xA0(OLAP) database.\\xA0This means that it is not intended to perform very\\xA0extremely-low-latency queries.\\xA0\"))), mdx(\"table\", null, mdx(\"thead\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"thead\"\n  }, mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"OLAP (Online Analytic Processing)\"), mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"center\"\n  }), \"OLTP (Online Transaction Processing)\"))), mdx(\"tbody\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Deals with Historical Data or Archival Data. OLAP is characterized by relatively low volume of transactions. Queries are often very complex and involve aggregations. For OLAP systems a response time is an effectiveness measure. OLAP applications are widely used by Data Mining techniques. In OLAP database there is aggregated, historical data, stored in multi-dimensional schemas (usually star schema). Sometime query need to access large amount of data in Management records like what was the profit of your company in last year.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"center\"\n  }), \"Is involved in the operation of a particular system. OLTP is characterized by a large number of short on-line transactions (INSERT, UPDATE, DELETE). The main emphasis for OLTP systems is put on very fast query processing, maintaining data integrity in multi-access environments and an effectiveness measured by number of transactions per second. In OLTP database there is detailed and current data, and schema used to store transactional databases is the entity model (usually 3NF). It involves Queries accessing individual record like Update your Email in Company database.\")))), mdx(\"h1\", null, \"Spark SQL CLI\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"You can make Spark SQL queries in local mode from the command line. It cannot communicate with the Thrift JDBC server. To start Spark SQL CLI, run the following in the spark directory:\\xA0\\n\", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"./bin/spark-sql\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"It is also possible to execute sql in an ad hoc manner via any of Spark's language by calling the method \\\"sql\\\" on the SparkSession object.\\n\", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"spark.sql(\\\"SELECT 1 + 1\\\").show()\"))), mdx(\"p\", null, \"Note: \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"You can completely interoperate between SQL and DataFrames e.g you can create a DataFrame, manipulate it with SQL, and then manipulate it again as a DataFrame.\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convert DataFrame to SQL:\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"spark.read.json(\\\"/data.json\\\")\\\\\\n.createOrReplaceTempView(\\\"some_sql_view\\\")\\xA0\\n\")), mdx(\"h1\", null, \"Catalog\"), mdx(\"p\", null, \"The highest level abstraction in Spark SQL is the Catalog. The Catalog is an abstraction for the storage of metadata about the data stored in your tables as well as other\\xA0helpful things like databases, tables, functions, and views. The catalog is available in\\xA0the org.apache.spark.sql.catalog.Catalog\\xA0package and contains a number of\\xA0helpful functions for doing things like listing tables, databases, and functions. We will\\xA0talk about all of these things shortly It\\u2019s very self explanatory to users, so we will omit\\xA0the code samples here but it\\u2019s really just another programmatic interface to Spark\\xA0SQL. This chapter shows only the SQL being executed; thus, keep in mind if you\\u2019re\\xA0using the programmatic interface that you need to wrap everything in a spark.sql\\xA0function call to execute the relevant code.\"), mdx(\"h1\", null, \"Tables\"), mdx(\"p\", null, \"To do anything useful with Spark SQL you first need to define tables. Tables are a structure of data against which you run commands. We can join tables, filter them, aggregate them, and perform different manipulations.\\nThe core difference between tables and DataFrames is that you define DataFrames in a scope of a programming language, whereas you define tables within a database. This means that when you create a table (assuming you never changed the database), it will belong to the default database.\\xA0\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"In Spark 2.X, tables always contain data. There is no notion of a temporary table: these are just views that do not contain data.  This is important because if you go to drop a table, you can risk losing the data when doing so.\")), mdx(\"h1\", null, \"Spark-Managed Tables\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"There are managed tables and unmanaged tables. \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"When you define a table from files on disk, you are defining an unmanaged table.\\xA0\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"When you use saveAsTable on a DataFrame you are creating a managed table for which Spark will keep track of all the relevant information for you. \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"This will read your table and write it out to a new location in Spark format. \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"You can see this reflected in the new explain plan. \", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"In the explain plan you will also notice that this writes to the default Hive warehouse location. You can set this by setting the spark.sql.warehouse.dir configuration to the directory of your choosing when you create your SparkSession.\")))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tables store two important information:\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"data within the tables\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"data about the tables, that is, metadata\")), mdx(\"h1\", null, \"Views\"), mdx(\"p\", null, \"Now that you created a table, another thing that you can define is a view.\\xA0\\nA view\\xA0specifies a set of transformations on top of an existing table\\u2014basically just saved\\xA0query plans, which can be convenient for organizing or reusing your query logic.\\xA0Spark has several different notions of views. Views can be global, set to a database, or\\xA0per session.\"), mdx(\"h1\", null, \"Creating Views\"), mdx(\"p\", null, \"To an end user, views are displayed as tables, except rather than rewriting all of the\\xA0data to a new location, they simply perform a transformation on the source data at\\xA0query time. This might be a filter, select, or potentially an even larger GROUP BY or\\xA0ROLLUP.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Views\"), \"\\nA view\\xA0specifies a set of transformations on top of an existing table\\u2014basically just saved\\xA0query plans, which can be convenient for organizing or reusing your query logic.\\xA0Spark has several different notions of views. Views can be global, set to a database, or\\xA0per session.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Creating Views\"), \"\\nTo an end user, views are displayed as tables, except rather than rewriting all of the\\xA0data to a new location, they simply perform a transformation on the source data at\\xA0query time. This might be a filter, select, or potentially an even larger GROUP BY or\\xA0ROLLUP.\"), mdx(\"hr\", null), mdx(\"h1\", null, \"Sourcers/Credits:\"), mdx(\"p\", null, \"To be completed\"));\n}\n;\nMDXContent.isMDXComponent = true;","tableOfContents":{"items":[{"url":"#the-spark-dataframe-and-sql-api","title":"The Spark DataFrame and SQL API"},{"url":"#fundamental-concepts","title":"Fundamental concepts:"},{"url":"#structured-api-overview","title":"Structured API Overview"},{"url":"#dataframes-and-datasets","title":"DataFrames and Datasets"},{"url":"#structured-apis","title":"Structured APIs"},{"url":"#overview-of-structured-api-execution","title":"Overview of Structured API Execution"},{"url":"#logical-planning","title":"Logical Planning"},{"url":"#physical-planning","title":"Physical Planning"},{"url":"#basic-structured-operations","title":"Basic Structured Operations"},{"url":"#dataframe","title":"DataFrame"},{"url":"#columns-and-expressions","title":"Columns and Expressions"},{"url":"#records-and-rows","title":"Records and Rows"},{"url":"#dataframe-api-example-using-different-types-of-functionality","title":"DataFrame API Example Using Different types of Functionality","items":[{"url":"#show","title":"show()"},{"url":"#shown","title":"show(n)"},{"url":"#take","title":"take()"},{"url":"#count","title":"count()"},{"url":"#head","title":"head()"},{"url":"#headn","title":"head(n)"},{"url":"#first","title":"first()"},{"url":"#collect","title":"collect()"},{"url":"#basic-dataframe-functions","title":"Basic DataFrame functions:"},{"url":"#printschema","title":"printSchema()"},{"url":"#todf","title":"toDF()"},{"url":"#dtypes","title":"dtypes()"},{"url":"#columns-","title":"columns ()"},{"url":"#cache","title":"cache()"},{"url":"#data-frame-operations","title":"Data Frame operations:"},{"url":"#sort","title":"sort()"},{"url":"#orderby","title":"orderBy()"},{"url":"#groupby","title":"groupBy()"},{"url":"#na","title":"na()"},{"url":"#as","title":"as()"},{"url":"#alias","title":"alias()"},{"url":"#select","title":"select()"},{"url":"#filter","title":"filter()"},{"url":"#where","title":"where()"},{"url":"#agg","title":"agg()"},{"url":"#limit","title":"limit()"},{"url":"#unionall","title":"unionAll()"},{"url":"#intersect","title":"intersect()"},{"url":"#except","title":"except()"},{"url":"#withcolumnrenamed","title":"withColumnRenamed()"},{"url":"#drop","title":"drop()"},{"url":"#dropduplicates","title":"dropDuplicates()"},{"url":"#describe","title":"describe()"},{"url":"#show-the-data","title":"Show the Data"},{"url":"#printschema-method","title":"printSchema Method"},{"url":"#select-method","title":"Select Method"},{"url":"#filter-method","title":"Filter Method"}]},{"url":"#working-with-different-types-of-data","title":"Working with Different Types of Data"},{"url":"#joins","title":"Joins"},{"url":"#challenges-when-using-joins","title":"Challenges When Using Joins"},{"url":"#data-sources","title":"Data Sources"},{"url":"#the-structure-of-the-data-sources-api","title":"The Structure of the Data Sources API"},{"url":"#basics-of-writing-data","title":"Basics of Writing Data"},{"url":"#save-mode","title":"Save Mode"},{"url":"#sparksql","title":"SparkSQL"},{"url":"#what-is-sql","title":"What is SQL?"},{"url":"#spark-sql","title":"Spark SQL"},{"url":"#spark-sql-cli","title":"Spark SQL CLI"},{"url":"#catalog","title":"Catalog"},{"url":"#tables","title":"Tables"},{"url":"#spark-managed-tables","title":"Spark-Managed Tables"},{"url":"#views","title":"Views"},{"url":"#creating-views","title":"Creating Views"},{"url":"#sourcerscredits","title":"Sourcers/Credits:"}]},"parent":{"__typename":"File","relativePath":"sparkAPI.md"},"frontmatter":{"metaTitle":"Week 6 - Apache Spark APIs","metaDescription":"Week 6 - Apache Spark APIs"}},"allMdx":{"edges":[{"node":{"fields":{"slug":"/internet","title":"Week 1- Internet"}}},{"node":{"fields":{"slug":"/bigdatahistory","title":"Big Data History [Optional Reading]"}}},{"node":{"fields":{"slug":"/syllabus","title":"Welcome - Syllabus"}}},{"node":{"fields":{"slug":"/resources","title":"Resources/Help"}}},{"node":{"fields":{"slug":"/apachespark","title":"Week 4 - Apache Spark"}}},{"node":{"fields":{"slug":"/spark-continued","title":"Week 7 - Apacke Spark Hands-on"}}},{"node":{"fields":{"slug":"/serverless","title":"Week 3 - Serverless"}}},{"node":{"fields":{"slug":"/sparkinternals","title":"Week 5 - Spark Internals"}}},{"node":{"fields":{"slug":"/sparkAPI","title":"Week 6 - Apache Spark APIs"}}},{"node":{"fields":{"slug":"/sparkinternals/1-sparkinternals","title":"Class Resources"}}},{"node":{"fields":{"slug":"/sparkinternals/3-sparkinternals","title":"Homework"}}},{"node":{"fields":{"slug":"/sparkinternals/2-sparkinternals","title":"Lecture Video"}}},{"node":{"fields":{"slug":"/sparkAPI/2-sparkAPI","title":"Lecture Video"}}},{"node":{"fields":{"slug":"/sparkAPI/3-sparkAPI","title":"Homework"}}},{"node":{"fields":{"slug":"/sparkAPI/1-sparkAPI","title":"Class Resources"}}},{"node":{"fields":{"slug":"/spark-continued/3-spark-continued","title":"Homework"}}},{"node":{"fields":{"slug":"/spark-continued/2-spark-continued","title":"Lecture Video"}}},{"node":{"fields":{"slug":"/spark-continued/1-spark-continued","title":"Class Resources"}}},{"node":{"fields":{"slug":"/serverless/1-serverless","title":"Class Resources"}}},{"node":{"fields":{"slug":"/serverless/2-serverless","title":"Lecture Video"}}},{"node":{"fields":{"slug":"/internet/1-internet","title":"Class Resources"}}},{"node":{"fields":{"slug":"/serverless/3-serverless","title":"Homework"}}},{"node":{"fields":{"slug":"/internet/3-internet","title":"Homework"}}},{"node":{"fields":{"slug":"/internet/2-internet","title":"Lecture Video"}}},{"node":{"fields":{"slug":"/aws/1-aws","title":"Class Resources"}}},{"node":{"fields":{"slug":"/aws/2-aws","title":"Lecture Video"}}},{"node":{"fields":{"slug":"/aws/3-homework","title":"Homework"}}},{"node":{"fields":{"slug":"/apachespark/1-apachespark","title":"Class Resources"}}},{"node":{"fields":{"slug":"/apachespark/2-apachespark","title":"Lecture Video"}}},{"node":{"fields":{"slug":"/apachespark/3-apachespark","title":"Homework"}}},{"node":{"fields":{"slug":"/aws","title":"Week 2 - AWS"}}},{"node":{"fields":{"slug":"/","title":"Data Engineering 3 - Big Data and Cloud Computing"}}}]}},"pageContext":{"id":"dcabc600-eff5-5241-943a-cad298f41d01"}},"staticQueryHashes":["2619113677","2619113677","3706406642","3706406642","417421954","417421954"]}